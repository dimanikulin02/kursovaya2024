{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11fc9dde-8575-49ab-b1e7-2cd34f5da40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\PycharmProjects\\machinelearninng\\venv\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 61.32307815551758\n",
      "Epoch: 0, Loss: 3.4262874126434326\n",
      "Epoch: 0, Loss: 3.0135669708251953\n",
      "Epoch: 0, Loss: 2.762882947921753\n",
      "Epoch: 0, Loss: 2.3703315258026123\n",
      "Epoch: 0, Loss: 1.9588171243667603\n",
      "Epoch: 0, Loss: 1.7172168493270874\n",
      "Epoch: 0, Loss: 1.8814373016357422\n",
      "Epoch: 0, Loss: 1.6060707569122314\n",
      "Epoch: 0, Loss: 1.696831464767456\n",
      "Epoch: 0, Loss: 1.5480051040649414\n",
      "Epoch: 0, Loss: 1.3724414110183716\n",
      "Epoch: 1, Loss: 1.4443495273590088\n",
      "Epoch: 1, Loss: 1.539047360420227\n",
      "Epoch: 1, Loss: 1.1785218715667725\n",
      "Epoch: 1, Loss: 1.2436654567718506\n",
      "Epoch: 1, Loss: 1.0143544673919678\n",
      "Epoch: 1, Loss: 1.061852216720581\n",
      "Epoch: 1, Loss: 1.093065619468689\n",
      "Epoch: 1, Loss: 0.7129022479057312\n",
      "Epoch: 1, Loss: 0.8494742512702942\n",
      "Epoch: 1, Loss: 0.733397901058197\n",
      "Epoch: 1, Loss: 0.8608652949333191\n",
      "Epoch: 1, Loss: 0.8365013599395752\n",
      "Epoch: 2, Loss: 0.5384209752082825\n",
      "Epoch: 2, Loss: 0.8266909718513489\n",
      "Epoch: 2, Loss: 0.7121158838272095\n",
      "Epoch: 2, Loss: 0.6443985104560852\n",
      "Epoch: 2, Loss: 0.49360576272010803\n",
      "Epoch: 2, Loss: 0.5131377577781677\n",
      "Epoch: 2, Loss: 0.5161886811256409\n",
      "Epoch: 2, Loss: 0.46114179491996765\n",
      "Epoch: 2, Loss: 0.458751380443573\n",
      "Epoch: 2, Loss: 0.48045504093170166\n",
      "Epoch: 2, Loss: 0.4535423815250397\n",
      "Epoch: 2, Loss: 0.3555583357810974\n",
      "Epoch: 3, Loss: 0.49792227149009705\n",
      "Epoch: 3, Loss: 0.3683873116970062\n",
      "Epoch: 3, Loss: 0.2913212180137634\n",
      "Epoch: 3, Loss: 0.23597781360149384\n",
      "Epoch: 3, Loss: 0.23527264595031738\n",
      "Epoch: 3, Loss: 0.3250301778316498\n",
      "Epoch: 3, Loss: 0.34334760904312134\n",
      "Epoch: 3, Loss: 0.37188735604286194\n",
      "Epoch: 3, Loss: 0.26168128848075867\n",
      "Epoch: 3, Loss: 0.3894236385822296\n",
      "Epoch: 3, Loss: 0.5398417115211487\n",
      "Epoch: 3, Loss: 0.3282936215400696\n",
      "Epoch: 4, Loss: 0.31311577558517456\n",
      "Epoch: 4, Loss: 0.22017207741737366\n",
      "Epoch: 4, Loss: 0.278304785490036\n",
      "Epoch: 4, Loss: 0.25251004099845886\n",
      "Epoch: 4, Loss: 0.1860770434141159\n",
      "Epoch: 4, Loss: 0.16153627634048462\n",
      "Epoch: 4, Loss: 0.17539745569229126\n",
      "Epoch: 4, Loss: 0.15687765181064606\n",
      "Epoch: 4, Loss: 0.25722435116767883\n",
      "Epoch: 4, Loss: 0.1582649052143097\n",
      "Epoch: 4, Loss: 0.2028372585773468\n",
      "Epoch: 4, Loss: 0.23111863434314728\n",
      "Epoch: 5, Loss: 0.19336508214473724\n",
      "Epoch: 5, Loss: 0.23228298127651215\n",
      "Epoch: 5, Loss: 0.20027609169483185\n",
      "Epoch: 5, Loss: 0.15884526073932648\n",
      "Epoch: 5, Loss: 0.12211152166128159\n",
      "Epoch: 5, Loss: 0.13124072551727295\n",
      "Epoch: 5, Loss: 0.1787605881690979\n",
      "Epoch: 5, Loss: 0.1234990730881691\n",
      "Epoch: 5, Loss: 0.12326765805482864\n",
      "Epoch: 5, Loss: 0.09363360702991486\n",
      "Epoch: 5, Loss: 0.16265439987182617\n",
      "Epoch: 5, Loss: 0.1915402114391327\n",
      "Epoch: 6, Loss: 0.19679555296897888\n",
      "Epoch: 6, Loss: 0.14141389727592468\n",
      "Epoch: 6, Loss: 0.08605897426605225\n",
      "Epoch: 6, Loss: 0.10534802079200745\n",
      "Epoch: 6, Loss: 0.1577494889497757\n",
      "Epoch: 6, Loss: 0.16883297264575958\n",
      "Epoch: 6, Loss: 0.15029527246952057\n",
      "Epoch: 6, Loss: 0.14023706316947937\n",
      "Epoch: 6, Loss: 0.11050871759653091\n",
      "Epoch: 6, Loss: 0.10468824207782745\n",
      "Epoch: 6, Loss: 0.22068311274051666\n",
      "Epoch: 6, Loss: 0.09796442091464996\n",
      "Epoch: 7, Loss: 0.1272697150707245\n",
      "Epoch: 7, Loss: 0.08096225559711456\n",
      "Epoch: 7, Loss: 0.09414104372262955\n",
      "Epoch: 7, Loss: 0.15440471470355988\n",
      "Epoch: 7, Loss: 0.15218035876750946\n",
      "Epoch: 7, Loss: 0.09440850466489792\n",
      "Epoch: 7, Loss: 0.09468184411525726\n",
      "Epoch: 7, Loss: 0.042041052132844925\n",
      "Epoch: 7, Loss: 0.09698671102523804\n",
      "Epoch: 7, Loss: 0.09862758219242096\n",
      "Epoch: 7, Loss: 0.08440860360860825\n",
      "Epoch: 7, Loss: 0.11383672058582306\n",
      "Epoch: 8, Loss: 0.06322961300611496\n",
      "Epoch: 8, Loss: 0.11319476366043091\n",
      "Epoch: 8, Loss: 0.08553744852542877\n",
      "Epoch: 8, Loss: 0.05024928227066994\n",
      "Epoch: 8, Loss: 0.07826840132474899\n",
      "Epoch: 8, Loss: 0.052199821919202805\n",
      "Epoch: 8, Loss: 0.07007038593292236\n",
      "Epoch: 8, Loss: 0.06840305775403976\n",
      "Epoch: 8, Loss: 0.06560574471950531\n",
      "Epoch: 8, Loss: 0.07562553137540817\n",
      "Epoch: 8, Loss: 0.06569156050682068\n",
      "Epoch: 8, Loss: 0.08354002982378006\n",
      "Epoch: 9, Loss: 0.06526724249124527\n",
      "Epoch: 9, Loss: 0.10660217702388763\n",
      "Epoch: 9, Loss: 0.10507760941982269\n",
      "Epoch: 9, Loss: 0.10768091678619385\n",
      "Epoch: 9, Loss: 0.1575184315443039\n",
      "Epoch: 9, Loss: 0.052725281566381454\n",
      "Epoch: 9, Loss: 0.05138203129172325\n",
      "Epoch: 9, Loss: 0.0468655489385128\n",
      "Epoch: 9, Loss: 0.09344427287578583\n",
      "Epoch: 9, Loss: 0.05276601389050484\n",
      "Epoch: 9, Loss: 0.04929709807038307\n",
      "Epoch: 9, Loss: 0.08241920918226242\n",
      "Epoch: 10, Loss: 0.11963848769664764\n",
      "Epoch: 10, Loss: 0.0822572335600853\n",
      "Epoch: 10, Loss: 0.10790397971868515\n",
      "Epoch: 10, Loss: 0.07938168942928314\n",
      "Epoch: 10, Loss: 0.06639070063829422\n",
      "Epoch: 10, Loss: 0.0572839230298996\n",
      "Epoch: 10, Loss: 0.0406542643904686\n",
      "Epoch: 10, Loss: 0.034554123878479004\n",
      "Epoch: 10, Loss: 0.08139392733573914\n",
      "Epoch: 10, Loss: 0.061850499361753464\n",
      "Epoch: 10, Loss: 0.05388728529214859\n",
      "Epoch: 10, Loss: 0.053061168640851974\n",
      "Epoch: 11, Loss: 0.08548851311206818\n",
      "Epoch: 11, Loss: 0.05605136603116989\n",
      "Epoch: 11, Loss: 0.06856448948383331\n",
      "Epoch: 11, Loss: 0.08741544187068939\n",
      "Epoch: 11, Loss: 0.056980524212121964\n",
      "Epoch: 11, Loss: 0.07368455082178116\n",
      "Epoch: 11, Loss: 0.07151472568511963\n",
      "Epoch: 11, Loss: 0.055279944092035294\n",
      "Epoch: 11, Loss: 0.08804155886173248\n",
      "Epoch: 11, Loss: 0.05108487978577614\n",
      "Epoch: 11, Loss: 0.07036854326725006\n",
      "Epoch: 11, Loss: 0.05056152492761612\n",
      "Epoch: 12, Loss: 0.036089807748794556\n",
      "Epoch: 12, Loss: 0.06313423067331314\n",
      "Epoch: 12, Loss: 0.06992652267217636\n",
      "Epoch: 12, Loss: 0.07802911102771759\n",
      "Epoch: 12, Loss: 0.08476404845714569\n",
      "Epoch: 12, Loss: 0.08453883230686188\n",
      "Epoch: 12, Loss: 0.05631241202354431\n",
      "Epoch: 12, Loss: 0.04840182140469551\n",
      "Epoch: 12, Loss: 0.08429592847824097\n",
      "Epoch: 12, Loss: 0.0401129350066185\n",
      "Epoch: 12, Loss: 0.07642769813537598\n",
      "Epoch: 12, Loss: 0.04541466757655144\n",
      "Epoch: 13, Loss: 0.0708799660205841\n",
      "Epoch: 13, Loss: 0.07979744672775269\n",
      "Epoch: 13, Loss: 0.048138897866010666\n",
      "Epoch: 13, Loss: 0.055988527834415436\n",
      "Epoch: 13, Loss: 0.05457352474331856\n",
      "Epoch: 13, Loss: 0.04253087565302849\n",
      "Epoch: 13, Loss: 0.06120314076542854\n",
      "Epoch: 13, Loss: 0.05602072551846504\n",
      "Epoch: 13, Loss: 0.032651204615831375\n",
      "Epoch: 13, Loss: 0.06404571235179901\n",
      "Epoch: 13, Loss: 0.07181984186172485\n",
      "Epoch: 13, Loss: 0.034604448825120926\n",
      "Epoch: 14, Loss: 0.05826730281114578\n",
      "Epoch: 14, Loss: 0.052554644644260406\n",
      "Epoch: 14, Loss: 0.06438596546649933\n",
      "Epoch: 14, Loss: 0.05305640026926994\n",
      "Epoch: 14, Loss: 0.043109241873025894\n",
      "Epoch: 14, Loss: 0.06026381626725197\n",
      "Epoch: 14, Loss: 0.06997402012348175\n",
      "Epoch: 14, Loss: 0.04850974306464195\n",
      "Epoch: 14, Loss: 0.04833005741238594\n",
      "Epoch: 14, Loss: 0.03609791770577431\n",
      "Epoch: 14, Loss: 0.08371192961931229\n",
      "Epoch: 14, Loss: 0.05934269353747368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "0: <|startoftext|>День рождения [SEP] С днем рождения! Желаю счастья, здоровья и всего наилучшего!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data = [\n",
    "    (\"С днем рождения! Желаю счастья, здоровья и всего наилучшего!\", \"День рождения\"),\n",
    "    (\"С Новым годом! Пусть этот год принесет вам радость и успех!\", \"Новый год\"),\n",
    "    (\"С 8 Марта! Желаю весны, любви и красоты в вашей жизни!\", \"Международный женский день\"),\n",
    "    (\"Счастливого Рождества! Пусть в вашем доме всегда будет тепло и уют!\", \"Рождество\"),\n",
    "    (\"С Днем защитника Отечества! Желаю мира, силы духа и верных друзей!\", \"День защитника Отечества\"),\n",
    "    # Добавление нескольких примеров для каждого праздника для лучшего обучения\n",
    "    (\"Поздравляю с Днем рождения! Пусть каждый новый день приносит радость и удовлетворение!\", \"День рождения\"),\n",
    "    (\"С наступающим Новым годом! Желаю, чтобы он был лучше предыдущего!\", \"Новый год\"),\n",
    "    (\"С 8 Марта! Желаю счастья, здоровья и крепкой любви!\", \"Международный женский день\"),\n",
    "    (\"Веселого Рождества! Пусть будет много смеха, радости и счастливых моментов!\", \"Рождество\"),\n",
    "    (\"Поздравляю с Днем защитника Отечества! Желаю стойкости, отваги и уверенности в себе!\", \"День защитника Отечества\"),\n",
    "     # Добавление новых примеров\n",
    "    (\"С днем рождения! Пусть этот день станет настоящим днем счастья!\", \"День рождения\"),\n",
    "    (\"С Новым годом! Желаю, чтобы этот год был плодотворным и наполнен успехом!\", \"Новый год\"),\n",
    "    (\"С 8 Марта! Пусть этот день принесет радость и веселье!\", \"Международный женский день\"),\n",
    "    (\"Счастливого Рождества! Пусть этот праздник будет для вас особенным!\", \"Рождество\"),\n",
    "    (\"С Днем защитника Отечества! Желаю вам сил и вдохновения!\", \"День защитника Отечества\"),\n",
    "    # Дополнительные примеры\n",
    "    (\"С днем рождения! Желаю вам здоровья, радости и веселья!\", \"День рождения\"),\n",
    "    (\"С Новым годом! Пусть этот год будет наполнен успехом и счастьем!\", \"Новый год\"),\n",
    "    (\"С 8 Марта! Желаю вам всего самого лучшего на этот праздник!\", \"Международный женский день\"),\n",
    "    (\"Счастливого Рождества! Пусть этот праздник принесет море подарков!\", \"Рождество\"),\n",
    "    (\"С Днем защитника Отечества! Чтобы Вы всегда с мужеством шли по дороге своей жизни, с честью и достоинством встречали все проблемы и решали их легко и просто. !\", \"День защитника Отечества\"),\n",
    "]*12\n",
    "\n",
    "\n",
    "# Подготовка данных\n",
    "class GreetingDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length=100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for item in data:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + item[1] + ' [SEP] ' + item[0] + '<|endoftext|>', \n",
    "                                       truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            \n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "max_length = 100\n",
    "dataset = GreetingDataset(tokenizer, data, max_length=max_length)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Настройка модели\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Обучение модели\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "model.train()\n",
    "\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_masks = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Сохранение модели\n",
    "model.save_pretrained(\"./gpt2_greetings\")\n",
    "tokenizer.save_pretrained(\"./gpt2_greetings\")\n",
    "\n",
    "# Генерация поздравления\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c9b7e89ae3c83994"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6035cb3b-75b3-4ff2-bb10-0562987b688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "0: <|startoftext|>Рождество [SEP] Веселого Рождества! Пусть этот праздник будет для вас особенным!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = \"<|startoftext|>Рождество [SEP]\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    generated, \n",
    "    do_sample=True,   \n",
    "    top_k=50, \n",
    "    max_length=100,\n",
    "    top_p=0.95, \n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(\"Generated Text:\\n\")\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    print(\"{}: {}\\n\".format(i, text))"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ca6749f-8335-4e1f-bf48-2d5ab0f19baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T13:05:05.360001Z",
     "start_time": "2024-09-22T13:05:05.356047Z"
    }
   },
   "source": "prompt = \"<|startoftext|>День рождения [SEP]\"",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
